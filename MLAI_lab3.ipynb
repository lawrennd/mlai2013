{
 "metadata": {
  "name": "MLAI_lab3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Lab 3: Generalization"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Neil D. Lawrence COM4509/6509 Machine Learning and Adaptive Intelligence"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Firstly, let's get the plots running in line on the  notebook.$$\\newcommand{\\inputScalar}{x}\\newcommand{\\lengthScale}{\\ell}\\newcommand{\\mappingVector}{\\mathbf{w}}\\newcommand{\\gaussianDist}[3]{\\mathcal{N}\\left(#1|#2,#3\\right)}\\newcommand{\\zerosVector}{\\mathbf{0}}\\newcommand{\\eye}{\\mathbf{I}}\\newcommand{\\dataStd}{\\sigma}\\newcommand{\\dataMatrix}{\\mathbf{Y}}\\newcommand{\\dataScalar}{y}\\newcommand{\\dataVector}{\\mathbf{y}}\\newcommand{\\inputVector}{\\mathbf{x}}\\newcommand{\\kernelMatrix}{\\mathbf{K}}\\newcommand{\\basisMatrix}{\\mathbf{\\Phi}}\\newcommand{\\expSamp}[1]{\\left<#1\\right>}\\newcommand{\\covarianceMatrix}{\\mathbf{C}}\\newcommand{\\numData}{N}\\newcommand{\\mappingScalar}{w}\\newcommand{\\mappingFunctionScalar}{f}$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The aim of this notebook is to review the different methods of model selection: hold out validation, leave one out cross validation and cross validation. The next thing to do is to download the olympics data just to make sure it is available for us to study."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "url = (\"http://staffwww.dcs.shef.ac.uk/\"\n",
      "    + \"people/N.Lawrence/dataset_mirror/\"\n",
      "    + \"olympic_marathon_men/olympicMarathonTimes.csv\")\n",
      "urllib.urlretrieve(url, 'olympicMarathonTimes.csv')\n",
      "olympics = np.loadtxt('olympicMarathonTimes.csv', delimiter=',')  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and as before extract both the olympic years and the pace of the winning runner into 2-dimensional arrays with the data points in the rows of the array (the first dimension)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = olympics[:, 0:1]\n",
      "y = olympics[:, 1:2]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can plot them to check that they've loaded in correctly.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x, y, 'rx')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Training Error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first thing we'll do is plot the training error for the polynomial fit. To do this let's set up some parameters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_data = x.shape[0]\n",
      "num_pred_data = 100 # how many points to use for plotting predictions\n",
      "x_pred = linspace(1890, 2016, num_pred_data)[:, None] # input locations for predictions\n",
      "order = 4 # The polynomial order to use.\n",
      "                 "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "now let's build the basis matrices.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# build the basis set\n",
      "Phi = np.zeros((num_data, order+1))\n",
      "Phi_pred = np.zeros((num_pred_data, order+1))\n",
      "for i in range(0, order+1):\n",
      "    Phi[:, i:i+1] = x**i\n",
      "    Phi_pred[:, i:i+1] = x_pred**i\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "now we can solve for the regression weights and make predictions both for the training data points, and the test data points. That involves solving the linear system given by"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\basisMatrix^\\top \\basisMatrix \\mappingVector^* = \\basisMatrix^\\top \\dataVector$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# solve the linear system\n",
      "w_star = np.linalg.solve(np.dot(Phi.T, Phi), np.dot(Phi.T, y))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and using the resulting vector to make predictions at the training points and test points,"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$\\mathbf{f} = \\basisMatrix \\mappingVector.$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To implement this in practice we need to use basis matrices for both the predictions and the training points."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# predict at training and test points\n",
      "f = np.dot(Phi, w_star)\n",
      "f_pred = np.dot(Phi_pred, w_star)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These can be used to compute the error"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$E(\\mappingVector) =  \\frac{\\numData}{2} \\log \\dataStd^2 + \\frac{1}{2\\dataStd^2} \\sum_{i=1}^\\numData \\left(\\dataScalar_i - \\mappingVector^\\top \\phi(\\inputVector_i)\\right)^2 \\\\\\\n",
      "E(\\mappingVector) = \\frac{\\numData}{2} \\log \\dataStd^2 + \\frac{1}{2\\dataStd^2} \\sum_{i=1}^\\numData \\left(\\dataScalar_i - \\mappingFunctionScalar_i\\right)^2$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the sum of squares term\n",
      "sum_squares = ((y-f)**2).sum()\n",
      "# fit the noise variance\n",
      "sigma2 = sum_squares/num_data\n",
      "error = 0.5*(num_data*np.log(sigma2) + sum_squares/sigma2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have the fit and the error, let's plot the fit and the error."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# print the error and plot the predictions\n",
      "print(\"The error is: %2.4f\"%error)\n",
      "plt.plot(x_pred, f_pred)\n",
      "plt.plot(x, y, 'rx')\n",
      "ax = plt.gca()\n",
      "ax.set_title('Predictions for Order 5')\n",
      "ax.set_xlabel('year')\n",
      "ax.set_ylabel('pace (min/km)')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now use the loop structure below to compute the error for different model orders.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# import the time model to allow python to pause.\n",
      "import time\n",
      "# import the IPython display module to clear the output.\n",
      "from IPython.display import clear_output\n",
      "\n",
      "error_list = []\n",
      "max_order = 6\n",
      "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
      "for order in range(0, max_order+1):\n",
      "    # 1. build the basis set\n",
      "    Phi = np.zeros((num_data, order+1))\n",
      "    Phi_pred = np.zeros((num_pred_data, order+1))\n",
      "    for i in range(0, order+1):\n",
      "        Phi[:, i:i+1] = x**i\n",
      "        Phi_pred[:, i:i+1] = x_pred**i\n",
      "    # 2. solve the linear system\n",
      "    # 3. make predictions at training and test points\n",
      "    # 4. compute the error and append it to a list.\n",
      "    # 5. plot the predictions\n",
      "    axes[0].clear()\n",
      "    axes[1].clear()    \n",
      "    axes[0].plot(x_pred, f_pred)\n",
      "    axes[0].plot(x, y, 'rx')\n",
      "    axes[0].set_ylim((2.5, 5.5))\n",
      "    axes[0].set_title('Predictions for Order ' + str(order) + ' model.')\n",
      "    axes[1].plot(np.arange(0, order+1), np.asarray(error_list))\n",
      "    axes[1].set_xlim((0, max_order))\n",
      "    axes[1].set_ylim((-30, 0))\n",
      "    axes[1].set_title('Training Error')\n",
      "    display(fig)\n",
      "    time.sleep(1)\n",
      "    clear_output()\n",
      "    \n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hold Out Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The error we computed above is the training error. It doesn't assess the model's generalization ability, it only assesses how well it's performing on the given training data. In hold out validation, we keep back some of the training data for assessing generalization performance. In the case of time series prediction, it often makes sense to hold out the last few data points, in particular, when we are interested in *extrapolation*, i.e. predicting into the future given the past. To perform hold out validation, we first remove the hold out set. If we were interested in interpolation, we would hold out some random points. Here, because we are interested in extrapolation, we will hold out all points since 1980. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a training set\n",
      "x_train = x\n",
      "y_train = y\n",
      "indices_hold_out = np.nonzero(x>1980)\n",
      "x_train = np.delete(x, indices_hold_out)\n",
      "y_train = np.delete(y, indices_hold_out)\n",
      "\n",
      "# Create a hold out set\n",
      "x_hold_out = x[indices_hold_out]\n",
      "y_hold_out = y[indices_hold_out]\n",
      "\n",
      "# Now use the training set and hold out set to compute the errors like above.\n",
      "\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now you have the training and hold out data, you should be able to use the code above to evaluate the model on the hold out data. Do this in the code block below."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Leave One Out Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In leave one out validation, you take out one data point at a time and compute the error. Perform leave one out validation on this data set. Test polynomial basis from 0th order to 6th order."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "$k$-fold Cross Validation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Repeat this test again for 5-fold cross validation. Here you need to split the data into 5 parts. Then use each part as a hold out validation set (in turn), whilst training on the four remaining parts. This is known as five fold cross validation. \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Do the different forms of validation select different models?"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}