{
    "1(a)":{
	"comment":"Here we were looking for the full error term including the portion that depends on $\\sigma^2$. Python does integer division if you write int/int. Since the size of the data is 27 (and an integer) 27/2=13, not 13.5. This is an irritating feature of the programming language, but should have been fixed. Loose one for failing to notice this. If this error is propagated through the whole analysis, but you only loose the mark once. You would also loose marks for using `np.linalg.inv` instead of `np.linalg.solve`.",
	"index":0,
	"marks":5
    },
    "1(b)":{
	"comment":"This is a straightforward question, it just involves replacing the basis provied (which is quadratic) instead of the linear basis. Only complexity is you need to compute the basis for the prediction points. This same task was seen in the second lab class.",
	"index":1,
	"marks":5
    },
    "1(c)":{
	"comment":"This is due to numerical problems. Range of weights required to add up the basis to make the whole thing come out around 3 to 5 would be very large. Computations on computers are best done around 1 for best numerical accuracy, but the coefficient associated with the 8th order term would have to be $\\mathcal{O}(2012^{-8})$ to keep the final contribution of that term in line with the others.",
	"index":2,
	"marks":5
    },
    "2(a)":{
	"comment":"This is also a straightforward warm up question. Again, the main thing to realise here is that the basis functions need to be computed across the entire input range. This has been practiced across the lab sessions, and example code is provided in the question.",
	"index":3,
	"marks":5
    },
    "2(b)":{
	"comment":"This has been done in lab classes and the the code is also provided. Should be a straightforward 5 marks.",
	"index":4,
	"marks":5
    },
    "2(c)":{
	"comment":"The noise variance needs to be fitted before computing the error here. And the error needs to be the one with the noise involved. Noise variance is found using code along the lines of `sigma2=((f-y)**2).mean()`",
	"index":5,
	"marks":10
    },
    "2(d)":{
	"comment":"The minimum error should be the one with 27 basis functions, as it should interpolate the data exactly. It's not a good fit.",
	"index":6,
	"marks":1
    },
    "2(e)":{
	"comment":"The hold out answer should be relatively few basis functions. If everything is implemented correctly it comes out as 1. Minor mistakes may make the number of basis one fewer or greater. Validation error should fluctuate a lot.",
	"index":7,
	"marks":10
    },
    "2(f)(i)":{
	"comment":"This is the first of the slightly harder questions. It should be minima at 6 basis functions. This was a requested piece of work for the polynomial basis functions in the 3rd lab class. It requires a bit of thought about the looping which is useful in teaching the principle of leave one out validation.",
	"index":8,
	"marks":10
    },
    "2(f)(ii)":{
	"comment":"Similar to the above, but slightly more difficult to think about. You want each fold to be balanced, but 27 doesn't divide by 5. This minor challenge was mentioned in lectures. The answer is for two of the hold out sets to contain 6 points. One small pitfall is for the validation sets to be overlapping. You need to make sure that the validation sets are distinct.",
	"index":9,
	"marks":10
    },
    "3(a)":{
	"comment":"This question is basically a test of the ability to map the marginal likelihood equation to a piece of code. Minor pit falls are if the student simply performs integer division, python has an annoying habit of returning an integer. Here you do need to use the inverse of $\\mathbf{K}$. We don't expect to see $\\sigma^2$ or $\\alpha$ fitted, although that would be the correct thing to do, we haven't covered non-linear optimisation in the course which is what would be required. The marginal likelihood should select a model with relatively few basis functions (3). If it hasn't then there's something wrong. One potential issue is that since this is a likelihood we are looking for a maximum, rather than a minimum we look for with an error.",
	"index":10,
	"marks":10
    },
    "3(b)":{
	"comment":"Here the posterior mean and covariance are given as equations above, the challenge is to interpret them correctly and combine them with the mapping function to get the correct predictions for the mean of $f$ and it's marginal standard deviation. Here we ask for the plot to be extended beyond the original range so that the behaviour of the fit can be seen outside the area of the data. If everything is correct then the basis functions will be seen to be dropping away from the data towards zero and initially error bars increase, but as you move further from the data error bars collapse.",
	"index":11,
	"marks":10
    },
    "3(c)":{
	"comment":"We covered sampling in this manner in the 4th lab class. The challenge here is only to use the multivariate normal sampler correctly, and make the correct use of the resulting samples. The covariance and mean have already been computed for the previous question.",
	"index":12,
	"marks":5
    },
    "3(d)":{
	"comment":"We are looking for some particular observations here. 1) The error bars are poor when there is no data, this is a very bad feature of fixed basis function models, and one that is fixed by using Gaussian processes which implicitly have infinite basis functions. 2) The error bars seem probably too large in general. This can be fixed a little by reducing the noise. But if you do this, then the model can become overly sensitive to the outlier (a larger number of basis functions will be selected so that the model can capture the 1904 outlier with their smaller width. This implies the need for a non-Gaussian likelihood, one which is less sensitive to outliers.",
	"index":13,
	"marks":9
    },
    "4(c)":{
	"comment":"",
	"index":15,
	"marks":0
    },
    "4(e)":{
	"comment":"",
	"index":14,
	"marks":0
    }
}