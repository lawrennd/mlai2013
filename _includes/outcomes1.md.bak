Learning Outcomes Week 1
------------------------

Understand that machine learning combines data with assumptions to make
predictions

Understand that probability provides a calculus of uncertainty for us to
deal with unknowns.

Understand the definition of entropy

Understand that the KL-divergence is an asymmetric measure of similarity
between probability distributions

Understand sample based approximations

Be able to prove that maximum likelihood solution is approximately
minimizing the KL-divergence

Be able to derive an error function from the likelihood of a single data
point.

-   Independence of data points (data is i.i.d.)
-   Logarithm is monotonic
-   In optimization, by convention, we minimize so take negative.

